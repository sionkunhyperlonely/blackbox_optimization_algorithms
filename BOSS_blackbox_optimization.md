# BOSS: Bandit-Optimized Surrogate Search  
**（ブラックボックス最適化アルゴリズム設計メモ）**

---

## 1. アルゴリズム概要

**名前（仮）**: **BOSS: Bandit-Optimized Surrogate Search**

### ターゲット問題

- 連続または混合変数のブラックボックス関数
- 評価コストが高い関数（1回の評価に秒〜分オーダー）
- 一定程度のノイズがあっても利用可能
- 次元は中程度（例: 5〜50 次元）

### 基本アイデア

1. **複数の局所サンプラ（ローカル探索器）を並列に用意**
   - 例:  
     - ランダム探索  
     - CMA-ES 風のガウスサンプラ  
     - TPE 風のサンプラ  
     - シンプレックス的な近傍探索 など
   - 各サンプラは「得意な探索パターン」を持つ。

2. **どのサンプラを使うかを「コンテキスト付きバンディット」で学習的に選択**
   - コンテキスト = 現在の探索状況（局所多様性、局所の目的関数分散、クラスタ情報など）。
   - 報酬 = サンプラが提案した点による「目的関数の改善量」。

3. **サロゲートモデルは“候補点のスコア補正”に限定して利用**
   - ガウス過程、ランダムフォレスト、XGBoost など任意の回帰モデルをサロゲートとして利用。
   - サンプラが生成した候補点に対して「予測値 + 不確実性 + 多様性」でスコアを付けて rerank する。
   - サロゲート単独で次点を最適化しないことで、モデル誤差に引きずられにくくする。

4. **グローバル探索 vs ローカル圧縮の自動バランス**
   - 候補点のクラスタリングや多様性指標から「探索が偏り過ぎているか」を判断。
   - 偏っていれば、グローバル志向のサンプラに有利な報酬スケーリングを行う。
   - 収束しつつある局所では、ローカル志向サンプラの報酬を強める。


---

## 2. 数学的前提と記法

- 最適化対象:  
  \( f: \mathcal{X} \to \mathbb{R} \)  
  （最小化を仮定。最大化は \(-f\) を最小化すればよい。）

- 探索空間:  
  \( \mathcal{X} \subset \mathbb{R}^d \)  
  （カテゴリ変数は one-hot などのエンコードで拡張可能。）

- 評価予算:  
  与えられた最大評価回数を \(T\) とする。


---

## 3. アルゴリズム詳細

### 3.1 初期化

1. **初期サンプル生成**
   - ラテン超方格サンプリング（LHS）などで、
     \( N_0 \) 点の初期点 \( \{x_i\}_{i=1}^{N_0} \) を生成。
   - 各点について \( y_i = f(x_i) \) を評価。
   - データセット \( \mathcal{D} = \{(x_i, y_i)\}_{i=1}^{N_0} \) を構成。

2. **サロゲートモデルの学習**
   - 任意の回帰モデル（例: RandomForest, XGBoost, GP）で
     \( y \approx M(x) \) を学習。

3. **サンプラ集合の定義**
   - サンプラ集合 \( \mathcal{S} = \{S_1, \dots, S_K\} \) を準備。
   - 各サンプラ \( S_k \) は、履歴 \( \mathcal{D} \)、中心点やパラメータを入力として、
     候補点集合を出力する「局所・準局所・大域探索器」とする。

   具体例:
   - \( S_1 \): 簡易 CMA-ES 風のガウス局所サンプラ
   - \( S_2 \): TPE 風「良い点周辺」重視サンプラ
   - \( S_3 \): 一様ランダムサンプラ（制約付き）
   - \( S_4 \): シンプレックスやランダムウォークベースのごく局所探索

4. **コンテキスト付きバンディットの初期化**
   - アーム数 = サンプラ数 \(K\)。
   - 各アームに対し、コンテキスト付きバンディット（LinUCB, Thompson Sampling など）の
     パラメータを初期化。
   - 報酬は「改善量を 0〜1 に正規化した値」を想定。


### 3.2 メインループ（1 ステップ）

反復回数 \( t = N_0 + 1, \dots, T \) について以下を繰り返す。

#### 3.2.1 コンテキストの構築

- 現在のベスト点を \( x^\* \)、その値を \( y^\* \) とする。
- \( x^\* \) 周辺の履歴点（ある半径内など）を集め、その局所情報から特徴量ベクトル
  \( c_t \) を作る。例:

  - 近傍の目的関数値の分散
  - 近傍に存在するサンプル数（密度）
  - 近傍におけるサロゲート予測勾配（有限差分近似でよい）
  - グローバルなクラスタ数、クラスタ間距離 など

- \( c_t \) をコンテキスト付きバンディットの「状態」として利用。

#### 3.2.2 サンプラ選択（コンテキスト付きバンディット）

- LinUCB や Contextual Thompson Sampling を用い、
  各サンプラ \( S_k \) に対して、コンテキスト \( c_t \) を与えたときの
  期待報酬 \( \hat{r}_k(c_t) \) を評価。
- UCB or サンプリングに基づき、1 つのサンプラ \( S_{k_t} \) を選択。

#### 3.2.3 候補点生成

- 選択されたサンプラ \( S_{k_t} \) に対して、
  - 履歴 \( \mathcal{D} \)
  - ベスト点 \( x^\* \)
  - 必要なら他のハイパーパラメータ
  を渡し、\( m \) 個の候補点
  \( \{x^{(j)}_{\text{cand}}\}_{j=1}^m \) を生成。

#### 3.2.4 サロゲートによるスコアリング／rerank

- 各候補点に対し、サロゲートモデル \( M \) による予測値と不確実性を得る:
  \[
    (\hat{y}^{(j)}, \sigma^{(j)}) = M(x^{(j)}_{\text{cand}})
  \]

- 最小化の場合のシンプルなスコア例:
  \[
    s^{(j)} = -\hat{y}^{(j)} + \lambda_t \cdot \sigma^{(j)} + \mu \cdot \mathrm{diversity\_bonus}(x^{(j)}_{\text{cand}}, \mathcal{D})
  \]

  - \( \lambda_t \): 時間とともに単調減少させる探索係数  
    （初期は不確実性重視、後半は予測値重視）。
  - \( \mu \): 多様性ボーナスの重み。
  - diversity\_bonus:  
    既存点との最小距離やクラスタ情報を用いた多様性指標。

- \( s^{(j)} \) が最大となる候補点（または上位数点）を実際に評価する点として選択。
  ここでは 1 点 \( x_t \) を選ぶとする。

#### 3.2.5 ブラックボックス評価

- 選択された \( x_t \) に対し、実評価 \( y_t = f(x_t) \) を行う。
- データセットを更新:
  \[
    \mathcal{D} \leftarrow \mathcal{D} \cup \{(x_t, y_t)\}
  \]

#### 3.2.6 バンディット報酬の更新

- 以前のベスト値を \( y_{\text{best, old}} \)、更新後のベスト値を \( y_{\text{best, new}} \) とする。
- 改善量:
  \[
    \Delta_t = \max(0, y_{\text{best, old}} - y_{\text{best, new}})
  \]
- \( \Delta_t \) を過去の改善量の分布などを使って \([0,1]\) に正規化し、
  正規化改善量 \( \tilde{\Delta}_t \) を報酬として、選択されたアーム \( S_{k_t} \) に与える。
- コンテキスト付きバンディットのパラメータを \( (c_t, \tilde{\Delta}_t) \) で更新。

#### 3.2.7 サロゲートの周期的リトレーニング

- 数ステップごとに、最新の \( \mathcal{D} \) を用いて \( M \) を再学習。
- 評価コストが高い前提では、学習コストは相対的に十分小さいと考えられる。


---

## 4. 擬似コード（Python風）

```python
def BOSS_optimize(f, bounds, T, N0=20, m=10, retrain_interval=10):
    # 初期サンプリング
    D = []  # list of (x, y)
    for _ in range(N0):
        x0 = sample_latin_hypercube(bounds)
        y0 = f(x0)
        D.append((x0, y0))

    # 初期サロゲート
    M = train_surrogate(D)

    # サンプラ群を定義
    samplers = [
        GaussianLocalSampler(bounds),
        TPEStyleSampler(bounds),
        GlobalRandomSampler(bounds),
        SimplexLocalSampler(bounds),
    ]
    bandit = ContextualBandit(num_arms=len(samplers))

    for t in range(N0, T):
        x_best, y_best = get_best(D)

        # 1. コンテキスト構築
        context = build_context(D, x_best)

        # 2. サンプラ選択（例: Thompson Sampling）
        k = bandit.select_arm(context)

        # 3. 候補生成
        candidates = samplers[k].sample_candidates(D, center=x_best, num=m)

        # 4. サロゲートでスコアリング
        scores = []
        for x_cand in candidates:
            y_hat, sigma = M.predict(x_cand)
            div_bonus = diversity_bonus(x_cand, D)
            score = -y_hat + lambda_t(t) * sigma + mu * div_bonus
            scores.append(score)

        # ベスト候補を選択
        idx = int(np.argmax(scores))
        x_next = candidates[idx]
        y_next = f(x_next)
        D.append((x_next, y_next))

        # 5. バンディット更新
        _, y_best_old = x_best, y_best
        _, y_best_new = get_best(D)
        delta = max(0.0, y_best_old - y_best_new)  # minimization
        reward = normalize_improvement(delta, history=D)
        bandit.update(k, context, reward)

        # 6. サロゲートの周期更新
        if (t - N0 + 1) % retrain_interval == 0:
            M = train_surrogate(D)

    return get_best(D)
```

---

## 5. 既存手法との違い・新規性

### 5.1 標準的ベイズ最適化 (BO) との違い

- 標準 BO は「サロゲート + 取得関数（EI, UCB など）」をグローバル最適化して次点を決める。
- BOSS では、
  - サロゲートはあくまで「サンプラが提案する候補の並べ替え・スコア補正」に限定。
  - 探索戦略の主役はサンプラ群とバンディットであり、
    サロゲートは“アシスト役”に近い。

### 5.2 進化戦略（CMA-ES など）との違い

- 進化戦略は通常、単一の分布（あるいは個体群）を更新していく。
- BOSS は
  - 多様なサンプラ（進化戦略的なもの、TPE風、ランダムなど）を並列に持ち、
  - バンディットにより「どのサンプラが今の問題に合っているか」をオンライン学習する。

### 5.3 Hyperband / BOHB 等との違い

- Hyperband/BOHB は「ハイパーパラメータ設定」をアームとして扱うことが多い。
- BOSS では、
  - アームは「サンプラ（探索戦略）」そのものであり、
  - サンプラが連続空間上に生成する点の性能に基づいて報酬を得る。

### 5.4 TPE との違い

- TPE は「良い領域」と「悪い領域」の密度比に基づき新点をサンプリングする単一モデル。
- BOSS では、
  - TPE 風サンプラも \(S_k\) の一員に過ぎず、
  - 他のサンプラ（CMA-ES 風、ランダム、局所探索など）と競合する。


---

## 6. 実装・実用面のポイント

### 6.1 実装のしやすさ

- サンプラ:
  - 既存の CMA-ES の簡略実装、ランダム探索、TPE などをそのまま利用して良い。
- バンディット:
  - LinUCB や Contextual Thompson Sampling の標準的実装を流用可能。
- サロゲート:
  - scikit-learn の `RandomForestRegressor` などで十分。

### 6.2 ハイパーパラメータ感度

- バンディット側の探索率や priors はある程度ラフでも動く設計が可能。
- \( \lambda_t \)（探索係数）は、
  - `lambda_t(t) = lambda0 / sqrt(t)` のような単調減少で十分。
- 多様性係数 \( \mu \) は、
  - 過度に大きくしない限り、局所解への早期収束を防ぐ方向に働く。

### 6.3 スケーラビリティ

- 評価コストが高い（秒〜分）ケースを主な想定とする。
- その場合、バンディット更新やサロゲート学習の計算コストは相対的に小さい。
- 評価が極端に安価な場合（ミリ秒オーダー）には、
  - オーバーヘッドが効いてしまうため、
  - サンプラ数やバンディット・サロゲートの更新頻度を下げるのが望ましい。


---

## 7. 今後の拡張の方向性

- サンプラ集合の自動生成・自動剪定：
  - 探索開始時に多数のサンプラを登録し、
  - 一定回数以上報酬が低ければアームごと削除するなど、
    メタレベルの進化戦略と組み合わせる。

- 問題特徴量を利用したメタ学習：
  - 問題の簡単な統計的特徴（次元数、制約の有無、ノイズレベルの推定など）と
    サンプラ性能の関係を学習し、
  - 新しい問題に対して初期のバンディットパラメータを良い状態に近づける。

- マルチ目的最適化への拡張：
  - サロゲートを多目的回帰にし、
  - サンプラ報酬を“パレートフロントへの貢献度”で定義するなどの拡張が考えられる。


---

以上が、BOSS (Bandit-Optimized Surrogate Search) による
「実用性を意識した新しいブラックボックス最適化アルゴリズム」の設計メモです。
このまま実装に落とし込むことも、特定用途（HPO、シミュレーション最適化など）向けに
カスタマイズすることもできます。
