# ATR-BO: Adaptive Trust-Region Bandit Optimization
（提案ブラックボックス最適化アルゴリズム / Markdown メモ）

## 1. 問題設定

評価コストが高く、勾配情報が利用できない「ブラックボックス関数」

\[
f: \mathcal{X} \rightarrow \mathbb{R}
\]

の最小化を考える。ここで

- \(\mathcal{X} \subset \mathbb{R}^d\)：連続または混合（連続 + 離散）な探索空間
- 評価 \(f(x)\) は高コスト・ノイズありでもよい
- 勾配 \(\nabla f(x)\) は利用しない（真のブラックボックス）

を想定する。

目的：限られた評価回数の中で、\(f(x)\) をできるだけ小さくするような \(x\) を見つける。

---

## 2. アルゴリズム概要

### 名称（仮）

**ATR-BO: Adaptive Trust-Region Bandit Optimization**

### コアアイデア

1. 探索空間を複数の「**信頼領域（trust region; TR）**」で覆い、それぞれを局所探索の単位として扱う。
2. 各信頼領域の内部で、
   - EI (Expected Improvement) 最大化
   - UCB (Upper Confidence Bound) 最大化
   - 一様ランダム
   - 局所ランダム
   など複数の**サンプリング戦略を「腕（arm）」として多腕バンディットで競わせる**。
3. サロゲートモデル（GP、ランダムフォレストなど）を用いて、
   - 各信頼領域の「有望度」
   - 領域内の不確実性
   を評価し、どの領域を重点的に探索するかを決める。
4. 領域ごとの最近の改善度に応じて、
   - 成功している領域は **半径を縮小**し、より局所探索へ
   - 改善の乏しい領域は **半径を拡大**または中心をシフトして大域探索寄りにする

これにより、

- 初期：複数の広い信頼領域で大域探索
- 後半：有望な領域のみにフォーカスして細かく局所探索

という自動的な探索モード遷移を実現する。

---

## 3. 信頼領域とバンディットの形式化

### 3.1 信頼領域（Trust Region, TR）

時刻 \(t\) における \(k\)-番目の信頼領域を

\[
\mathcal{R}_{k}^{(t)} = \{ x \in \mathcal{X} \mid \lVert x - c_k^{(t)} \rVert_{\infty} \le r_k^{(t)} \}
\]

と定義する。

- \(c_k^{(t)}\)：領域の中心
- \(r_k^{(t)}\)：領域半径（\(L_\infty\) ノルムでのハイパーキューブを想定）

信頼領域は、局所探索の「箱」として機能する。各 TR は次の情報を内部に持つ：

- 中心 \(c_k^{(t)}\)
- 半径 \(r_k^{(t)}\)
- 最近の評価履歴（点と値）
- 改善量履歴（後述）

### 3.2 領域内バンディット（Multi-Armed Bandit）

各信頼領域 \(\mathcal{R}_k\) 内では複数のサンプリング戦略（腕）を用意する。例：

- Arm 1：サロゲートモデルに基づく **EI 最大化**
- Arm 2：サロゲートモデルに基づく **UCB 最大化**
- Arm 3：**一様ランダムサンプリング**
- Arm 4：**中心付近への局所ランダムサンプリング**

領域 \(k\) におけるアーム \(a\) の報酬は、「そのアームでサンプルした点によって得られた改善量」とする。

改善量 \(\Delta f\) を

\[
\Delta f = f(x_{\text{best, before}}) - f(x_{\text{new}})
\]

と定義する（最小化問題なので、値が小さくなるほど改善）。

領域 \(k\) のアーム \(a\) に対し、以下の統計量を保持する：

- \(\bar{r}_{k,a}^{(t)}\)：平均報酬（平均改善量）
- \(n_{k,a}^{(t)}\)：アーム \(a\) を選択した回数
- \(N_k^{(t)}\)：領域 \(k\) での総試行回数 \(N_k^{(t)} = \sum_a n_{k,a}^{(t)}\)

アーム選択には UCB1 型のスコアを用いる：

\[
\text{score}_{k,a}^{(t)}
= \bar{r}_{k,a}^{(t)} + \beta \sqrt{\frac{\ln N_k^{(t)}}{n_{k,a}^{(t)} + 1}}
\]

- \(\beta > 0\)：探索バイアスを調整するハイパーパラメータ

---

## 4. サロゲートモデルと箱のスコアリング

### 4.1 サロゲートモデル \(M\)

観測データ集合

\[
D = \{(x_i, y_i)\}_{i=1}^n, \quad y_i = f(x_i)
\]

に対して、サロゲートモデル \(M\) を学習する。

- 低次元（例：\(d<10\)）ではガウス過程回帰（GPR）
- 中〜高次元ではランダムフォレスト、TPE 的モデルなどを想定

モデルから予測値 \(\hat{f}(x)\) と不確実性 \(\sigma(x)\)（分散など）を取得する。

### 4.2 箱の有望度スコア

領域 \(k\) に対し、サロゲートモデルを用いて、

- 領域内の予測最良値（近似）

  \[
  \hat{f}_{k,\min}^{(t)} \approx \min_{x \in \mathcal{R}_k^{(t)}} \hat{f}(x)
  \]

- 領域内の平均的な不確実性

  \[
  u_k^{(t)} \approx \mathbb{E}_{x \in \mathcal{R}_k^{(t)}}[\sigma(x)]
  \]

を計算する（厳密最適化でなく、サンプリングベースの近似で十分）。

箱のスコアを

\[
\text{box\_score}_k^{(t)} = -\hat{f}_{k,\min}^{(t)} + \gamma u_k^{(t)}
\]

で定義する。

- \(-\hat{f}_{k,\min}^{(t)}\)：予測最良値が小さいほどスコアが高い
- \(\gamma u_k^{(t)}\)：不確実性の高い領域も探索するためのボーナス

反復ごとに、スコアが高い上位 \(K_{\text{box}}\) 個の領域を **アクティブ領域** として選択し、その中で評価を行う。

---

## 5. アルゴリズムのフロー

### 5.1 初期化

1. **初期サンプリング**
   - 探索空間 \(\mathcal{X}\) に対して、LHS（Latin Hypercube Sampling）等で \(n_0\) 点をサンプル。
   - 各点で \(f(x)\) を評価し、データ集合 \(D\) を作成。

2. **サロゲートモデルの学習**
   - 初期データ \(D\) からサロゲートモデル \(M\) を学習。

3. **信頼領域の初期化**
   - 初期データから良好な値を持つ点をいくつか選び、その周囲に
     \(K_{\text{init}}\) 個の信頼領域を生成。
   - 各 TR に対して
     - 中心 \(c_k^{(0)}\)
     - 初期半径 \(r_k^{(0)}\)
     - バンディット統計（アームごとの報酬、試行回数）
     を初期化。

### 5.2 反復ステップ（t = 1, 2, ..., T）

1. **箱のスコアリング**
   - 各領域 \(k\) について、サロゲートモデルから \(\hat{f}_{k,\min}^{(t)}, u_k^{(t)}\) を推定し、
     \(\text{box\_score}_k^{(t)}\) を計算。
   - スコア上位の \(K_{\text{box}}\) 個をアクティブ領域とする。

2. **アクティブ領域内でのサンプリング**
   - 各アクティブ領域 \(k\) について：
     1. バンディット式（UCB1）によりアーム \(a\) を1つ選択。
     2. 選ばれたアームの戦略に従い、領域 \(\mathcal{R}_k^{(t)}\) 内から新規点 \(x_{k}^{(t)}\) を生成。
     3. 実際に関数評価を行い、\(y_{k}^{(t)} = f(x_{k}^{(t)})\) を得る。
     4. 改善量 \(\Delta f\) を報酬としてバンディット統計を更新。
     5. \((x_{k}^{(t)}, y_{k}^{(t)})\) をデータ集合 \(D\) に追加。

3. **信頼領域の更新**
   - 各アクティブ領域 \(k\) について、最近数ステップの改善量の平均

     \[
     \overline{\Delta f}_k^{(t)} = \text{mean of recent } \Delta f
     \]

     を計算。
   - 閾値 \(\varepsilon\) と比較し、
     - \(\overline{\Delta f}_k^{(t)} > \varepsilon\) なら「成功」とみなし、**半径縮小**：

       \[
       r_k^{(t+1)} = \alpha_{\text{shrink}} r_k^{(t)}, \quad 0 < \alpha_{\text{shrink}} < 1
       \]

     - \(\overline{\Delta f}_k^{(t)} \le \varepsilon\) なら「停滞」とみなし、**半径拡大または中心移動**：

       \[
       r_k^{(t+1)} = \alpha_{\text{expand}} r_k^{(t)}, \quad \alpha_{\text{expand}} > 1
       \]

       - 改善がほとんど無い場合や極端に悪い場合には、グローバルに有望な点の近傍へ中心をリセットするなどのルールを追加してもよい。

4. **サロゲートモデルの更新**
   - 一定ステップごと（例：\(t \bmod T_{\text{sur}} = 0\)）に、最新のデータ集合 \(D\) に基づいてサロゲートモデル \(M\) を再学習。

5. **終了条件**
   - 総評価回数が所定の上限に達した
   - しばらく改善が見られない
   - すべての信頼領域の半径が十分小さくなった

   などの条件を満たしたら停止し、\(D\) 中の最良点を解として返す。

---

## 6. 擬似コード

連続最小化問題を想定した擬似コード例。

```text
Input: domain X, total budget T, initial_samples n0
Output: x_best

D = sample_initial_points(X, n0)
evaluate f for all x in D
M = fit_surrogate(D)

R = init_trust_regions(D, K_init)  // each region has center c_k, radius r_k
init_bandits(R)                    // bandit stats for each (region k, arm a)

for t = 1..T:

    // 1. box scoring
    for each region k in R:
        f_hat_min_k = approx_min_in_region(M, R_k)
        u_k = uncertainty_in_region(M, R_k)
        box_score_k = -f_hat_min_k + gamma * u_k
    active_regions = top_K(R, box_score_k)

    new_points = []
    for each region k in active_regions:
        a = select_arm_UCB(k)              // use bandit stats (UCB1)
        x_new = sample_point_in_region(M, R_k, a)
        y_new = f(x_new)
        append (x_new, y_new) to D
        update_bandit_stats(k, a, y_new)
        update_region_local_stats(R_k, x_new, y_new)

    // 3. trust region update
    for each region k in active_regions:
        if mean_recent_improvement(R_k) > epsilon:
            R_k.radius *= alpha_shrink
        else:
            R_k.radius *= alpha_expand
            maybe_shift_center(R_k, D, M)

    // 4. surrogate update (periodically)
    if t mod T_surrogate_update == 0:
        M = refit_surrogate(D)

return argmin_x_in_D f(x)
```

---

## 7. 既存手法との違い（高レベル）

### 7.1 CMA-ES との違い

- CMA-ES：
  - 単一（または少数）の多変量正規分布を進化させる。
  - 「分布そのもの」を進化戦略で更新する。

- ATR-BO：
  - 複数の **信頼領域（ハイパーキューブ）** をもつ。
  - 各領域内で複数のサンプリング戦略をバンディットで切り替える。
  - サロゲートモデルは「領域スコアリング」用のグローバルな情報源として使う。

### 7.2 標準的なベイズ最適化（GP + EI/UCB）との違い

- 標準 BO：
  - 単一のサロゲートモデル + 単一の獲得関数で点を逐次選ぶ。

- ATR-BO：
  - サロゲートは「大域的な地図」として使い、
  - 実際のサンプリングは
    - 「どの信頼領域を重視するか」
    - 「領域の中でどの戦略（arm）を使うか」
    を別々に決める二段構え。

### 7.3 Trust-Region BO 系との違い

- 既存の TR-BO の多くは、1つの TR を維持し成功/失敗に応じてサイズを変える構造。
- ATR-BO は、
  - 複数 TR を同時に持ち、
  - 各 TR 内で多腕バンディットを回す
  という構造的な違いがある。

---

## 8. ハイパーパラメータと実用上の指針

### 8.1 主なハイパーパラメータ

- \(K_{\text{init}}\)：初期信頼領域の数
- \(K_{\text{box}}\)：各ステップでアクティブにする領域の数
- \(\alpha_{\text{shrink}}\)：半径縮小係数（例：0.5〜0.8）
- \(\alpha_{\text{expand}}\)：半径拡大係数（例：1.2〜1.5）
- \(\beta\)：領域内バンディットの探索係数（例：0.5〜2）
- \(\gamma\)：箱スコアにおける不確実性ボーナスの重み
- \(\varepsilon\)：領域成功判定に用いる改善量閾値
- \(T_{\text{sur}}\)：サロゲート再学習の周期

### 8.2 チューニングの実務的な目安

- **評価コストが非常に高い場合**：
  - \(K_{\text{init}}, K_{\text{box}}\) を小さめ（3〜5）にして、個々の領域に十分な評価を割く。
- **次元数が中程度（10〜50）**：
  - サロゲートは GPR よりランダムフォレスト系のほうが扱いやすい。
- **局所解にハマりやすい問題**：
  - \(\gamma\) を大きめにして、未探索の領域（高不確実性）のスコアを引き上げる。

---

## 9. 想定される強みと弱み

### 9.1 強み

- **局所探索と大域探索を直感的に制御可能**：
  - TR の数とサイズ、更新ルールを調整することで、問題に合わせたバランスが取りやすい。
- **複数の獲得関数・戦略の自動選択**：
  - EI, UCB, ランダム等をバンディットで自動的に重み付けでき、
  - 「どの獲得関数を使うか」をユーザが事前に固定しなくてよい。
- **並列評価に向いている**：
  - 複数の領域 × 複数アームから一度に候補点を生成できる。

### 9.2 弱み / 課題

- 実装がやや複雑
  - TR 管理、バンディット、サロゲートの再学習など複数のモジュールが必要。
- 理論解析の難しさ
  - 多腕バンディット・TR・ベイズ最適化の要素が混在するため、
    厳密な後悔界などの理論保証を得るには工夫が必要。

---

## 10. 実装に向けたステップ案

1. **低次元のテスト関数で検証**
   - Branin, Hartmann などのベンチマーク関数で
     - まずは 1 つの TR + バンディット構造で動作確認。
2. **サロゲートを簡易に**
   - 最初はランダムフォレスト + EI のみを「EI 腕」として実装し、
   - UCB 腕、ランダム腕をあとから追加。
3. **複数 TR への拡張**
   - TR を複数に増やし、箱スコアリングによるアクティブ領域選択を導入。
4. **並列化**
   - 領域ごとに複数点を同時サンプルして、実験環境に合わせてバッチ最適化を行う。

---

## 11. まとめ

ATR-BO は、

- 「複数の信頼領域による空間分割」
- 「領域内での多腕バンディットによるサンプリング戦略選択」
- 「サロゲートモデルを使った箱スコアリング」

を組み合わせた、ブラックボックス最適化のためのメタ戦略である。

- 高コスト評価問題
- 次元が中程度
- 勾配が利用できない状況

などで実用的に使える可能性があり、従来の単一 BO や CMA-ES と比較して、

- 探索モードの切り替え
- 複数戦略の自動選択
- 並列評価のしやすさ

という点で利点を持つ設計となっている。
