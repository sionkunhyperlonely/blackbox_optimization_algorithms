# 疑似グラディエント適応サンプリング (PGAS) によるブラックボックス最適化アルゴリズム

## 1. 概要

本ドキュメントでは、既存の代表的手法（Bayesian Optimization, CMA-ES, NES, GA, Nelder–Mead など）と構造的に異なる、
**Pseudo-Gradient Adaptive Sampling (PGAS; 疑似グラディエント適応サンプリング)** というブラックボックス最適化アルゴリズムを定義します。

PGAS は、以下の特徴を持つアルゴリズムです。

- 勾配情報を用いない **ブラックボックス最適化** 向け
- 評価値の **順位情報** から「疑似グラディエント方向」を推定
- 複数の局所クラスタごとに **サンプリング楕円体（ガウス分布）」を持ち、疑似グラディエントに基づいてそれらを移動・変形**
- 多峰性のある問題にもある程度対応可能（クラスタが複数のモードを担当）
- 実装は線形代数とクラスタリング、ランダムサンプリングが中心で比較的シンプル

※ここで示すアルゴリズムは、「一般に広く知られている手法とは明確に構造が異なる」という意味での「新規性」を狙った設計です。
世界中の未発表研究も含めて「完全に初めて」と断定することはできません。

---

## 2. 問題設定

- 最適化したい関数：  
  \( f: \mathbb{R}^d \to \mathbb{R} \)（最大化を想定）
- 特徴：
  - 関数の解析的な形は不明
  - 勾配 \( \nabla f \) は利用不可能
  - 1回の評価コストが高いことを想定（物理シミュレーション、実験など）
- 設計領域：  
  \( \mathcal{X} \subset \mathbb{R}^d \)（典型的にはボックス制約）
- 評価回数制限：  
  最大評価回数 \( N_\text{eval} \) が与えられているとする。

---

## 3. アルゴリズムのコアアイデア

PGAS の中核となる考え方は次の通りです。

> **局所クラスタ内のサンプルの「相対順位」から疑似グラディエントを構成し，
> その方向に沿ってクラスタのサンプリング分布（楕円体ガウス）を変形していく。**

より具体的には：

1. 評価済み点を複数の **局所クラスタ** に分ける。
2. 各クラスタ内で、評価値の高いサンプルを「良い点」とみなし、
   その点群の重心方向を **疑似グラディエント** とする。
3. 疑似グラディエントに従って、クラスタ中心および楕円体の形状（共分散に相当）を更新。
4. 更新された楕円体から新しいサンプルを生成し、評価値に応じて履歴を更新。
5. 全体として、複数の楕円体が「良さそうな方向」に徐々に引き寄せられつつ、
   ある程度のランダム探索も維持する。

このように、

- BO のようにサロゲートモデルを構築しない
- CMA-ES のように **単一グローバル分布** の共分散を最適化しない
- Nelder–Mead のような単一シンプレックス変形とも異なる

という点で差別化されています。

---

## 4. アルゴリズム定義：PGAS

### 4.1 ハイパーパラメータ

- \( K \)：同時に維持する **クラスタ数**（局所領域の数）
- \( m \)：各クラスタが保持する履歴点数（バッファサイズ）
- \( n_\text{init} \)：初期サンプル数
- \( \alpha \in (0,1] \)：クラスタ中心の更新ステップサイズ
- \( \beta \in (0,1] \)：サンプリング楕円体の伸縮強度
- \( \gamma \in (0,1) \)：探索モードに入る確率（探索 vs 利用 のバランス）
- \( \sigma_\text{min}, \sigma_\text{max} \)：楕円体の最小・最大スケール（固有値の下限・上限）

### 4.2 内部状態

各クラスタ \( k = 1, \dots, K \) について、以下を保持する：

- 中心 \( c_k \in \mathbb{R}^d \)
- 形状行列 \( S_k \in \mathbb{R}^{d \times d} \)：対称正定値。楕円体の共分散に相当。
- 履歴集合 \( H_k = \{(x_i, f_i)\}_{i=1}^{\le m} \)：そのクラスタに属するとみなしている評価済み点と目的関数値。

---

## 5. アルゴリズム詳細ステップ

### 5.1 初期化

1. **初期サンプリング**
   - ラテン超方格サンプリング（Latin Hypercube Sampling）や単純な一様サンプリングで、
     \( n_\text{init} \) 個の点 \( x^{(1)}, \dots, x^{(n_\text{init})} \in \mathcal{X} \) を生成。
2. **評価**
   - 各点を評価し、\( f(x^{(i)}) \) を得る。
3. **クラスタリング**
   - K-means などのクラスタリング手法で、これらの点を \( K \) クラスタに分割。
4. **クラスタ初期化**
   - 各クラスタ \( k \) に対して：
     - \( c_k \) をクラスタの重心に設定。
     - \( S_k \) をクラスタ点群の共分散行列に小さな正則化項 \( \varepsilon I \) を加えたものに設定。
     - \( H_k \) にクラスタ所属点と評価値を格納（最大 \( m \) 点、超えたら古い順に削除）。

---

### 5.2 局所疑似グラディエントの推定

クラスタ \( k \) の履歴 \( H_k = \{(x_i, f_i)\} \) から疑似グラディエントを構成する。

1. **局所ランキング**
   - \( H_k \) 内で \( f_i \) の降順（大きい方が良い）にソートし、
     上位 \( q \% \) の点（例：上位 30%）を「良い点」集合 \( G_k \) とする。
2. **方向ベクトルの計算**
   - 各良い点と中心の差分をとる：  
     \[ v_j = x_j - c_k \quad ((x_j, f_j) \in G_k) \]
   - それらの平均を疑似グラディエント方向とみなす：  
     \[ g_k = \frac{1}{|G_k|} \sum_{(x_j, f_j) \in G_k} v_j \]
3. **正規化とランダム性の混合**
   - \( \|g_k\| \) が十分に大きいとき：\( g_k \) を単位ベクトルに正規化し必要に応じてスケーリング。
   - \( \|g_k\| \) が非常に小さいとき：局所的に「勾配がほぼない」と判断し、ランダムな単位ベクトルを混合する。

これにより、**評価値の高い点の方向に向かう平均的なベクトル**として疑似グラディエントを得る。

---

### 5.3 サンプリング楕円体（分布）の更新

クラスタ \( k \) に対して：

1. **中心の更新（シフト）**
   - ステップサイズ \( \alpha \) を使って、
     \[ c_k \leftarrow \Pi_\mathcal{X}\bigl(c_k + \alpha \tilde{g}_k\bigr) \]
     ここで \( \tilde{g}_k \) は正規化後の疑似グラディエント、\( \Pi_\mathcal{X} \) は制約領域 \( \mathcal{X} \) への射影。
2. **形状行列の更新（伸縮）**
   - 形状行列 \( S_k \) を、疑似グラディエント方向には分散をやや縮小し、
     直交方向にはやや広げるイメージで更新する。例として：  
     \[
       S_k \leftarrow (1+\beta) S_k - \beta \cdot
       \frac{S_k \tilde{g}_k \tilde{g}_k^\top S_k}{\tilde{g}_k^\top S_k \tilde{g}_k}
     \]
   - 更新後、固有値分解 \( S_k = U \Lambda U^\top \) を行い、
     各固有値を \( [\sigma_\text{min}^2, \sigma_\text{max}^2] \) にクリップして数値安定性と探索スケールを制御する。

これにより、クラスタは「良さそうな方向に移動しつつ、その方向にはあまり広がり過ぎない」サンプリング分布へと変形される。

---

### 5.4 サンプリングと評価

各イテレーションでの新規サンプル生成と評価：

1. **クラスタの選択**
   - 確率 \( \gamma \) で **探索モード**：
     - ランダムにクラスタを選ぶ、あるいは一様分布からサンプルする。
   - 確率 \( 1 - \gamma \) で **利用モード**：
     - 各クラスタの過去のベスト値に比例した確率でクラスタを選択（高性能クラスタを優遇）。
2. **サンプリング**
   - 選択したクラスタを \( k \) とし、Cholesky 分解 \( S_k = L_k L_k^\top \) を用いて
     \[
       z \sim \mathcal{N}(0, I_d), \quad
       x_\text{new} = c_k + L_k z
     \]
   - 得られた \( x_\text{new} \) を制約領域 \( \mathcal{X} \) に射影。
3. **評価**
   - \( f(x_\text{new}) \) を評価し、評価回数カウンタ \( N \) を更新。
4. **クラスタへの割り当て**
   - 現在のクラスタ中心 \( \{c_1, \dots, c_K\} \) のうち、最も近い中心をもつクラスタを \( k^* \) とする。
   - \( (x_\text{new}, f(x_\text{new})) \) を \( H_{k^*} \) に追加し、サイズが \( m \) を超えた場合は古いデータを削除。
   - 各クラスタの「局所ベスト値」および全体の「グローバルベスト値」を更新。

---

### 5.5 再クラスタリング（オプション）

- 一定イテレーションごとに、全ての履歴点を使って再度クラスタリングを行うことで、
  クラスタと楕円体を最新の分布にリフレッシュすることができる。
- 動的にモード構造が変化するような問題や、初期クラスタリングの質があまり良くない場合に有効。

---

### 5.6 停止条件

以下のいずれかを満たした時点でアルゴリズムを停止する：

- 評価回数が最大 \( N_\text{eval} \) に達した。
- ある連続イテレーション数の間、グローバルベスト値が更新されなかった。
- 全クラスタの楕円体のスケール（固有値）が \( \sigma_\text{min} \) 付近まで縮んだ。

最終的な解として、履歴全体の中で最大の \( f(x) \) を与える点を出力する。

---

## 6. 擬似コード

以下は PGAS アルゴリズムの簡略擬似コードです。

```pseudo
Input: domain X, dimension d
Hyperparams: K, m, n_init, α, β, γ, σ_min, σ_max, N_eval

# --- initialization ---
D = LatinHypercubeSample(X, n_init)
Evaluate f(x) for x in D
Cluster D into K clusters (k-means etc.)
For each cluster k:
    c_k = cluster centroid
    S_k = covariance(cluster points) + ε I
    H_k = points in this cluster (with f-values)

best_x, best_f = argmax over all evaluated points

eval_count = n_init

# --- main loop ---
while eval_count < N_eval:

    # choose cluster index k
    if rand() < γ:
        # exploration: random cluster
        k = random choice among {1..K}
    else:
        # exploitation: probability proportional to best f in each cluster
        k = weighted_choice(clusters, weights=cluster_best_f)

    # compute pseudo-gradient g_k using H_k
    G_k = top q% points in H_k by f
    if |G_k| >= 2:
        g_k = average(x - c_k for x in G_k)
        if norm(g_k) < ε_g:
            g_k = random_unit_vector()
        else:
            g_k = normalize(g_k)
    else:
        g_k = random_unit_vector()

    # update center
    c_k = project_to_domain(c_k + α * g_k, X)

    # update shape matrix S_k
    S_k = update_shape_matrix(S_k, g_k, β)
    S_k = clip_eigenvalues(S_k, σ_min^2, σ_max^2)

    # sample new point from cluster k
    L_k = cholesky(S_k)
    z ~ N(0, I_d)
    x_new = project_to_domain(c_k + L_k * z, X)

    # evaluate
    f_new = f(x_new)
    eval_count += 1

    # assign new point to nearest cluster
    k_assigned = nearest_cluster_center(x_new, {c_1..c_K})
    add (x_new, f_new) to H_{k_assigned}, keeping |H_{k_assigned}| <= m

    # update stats
    update_cluster_best(k_assigned, x_new, f_new)
    if f_new > best_f:
        best_f = f_new
        best_x = x_new

    # (optional) re-cluster occasionally

return best_x, best_f
```

---

## 7. 既存手法との違い

### 7.1 Bayesian Optimization と比較

- BO はガウス過程などのサロゲートモデルを構築し、獲得関数（EI, UCB など）の最大化によって次点を決定する。
- PGAS はサロゲートモデルを明示的には使わず、
  **局所クラスタ内の順位情報のみから疑似グラディエントを構成し、サンプリング分布そのものを変形**する。
- 高次元での GP の計算負荷や、獲得関数最大化の難しさを避けられる可能性がある。

### 7.2 CMA-ES / NES / Cross-Entropy Method と比較

- これらは主に **単一のグローバル分布**（ガウス分布等）の平均・共分散を更新する分布進化アルゴリズムである。
- PGAS は、**複数クラスタに対してそれぞれ楕円体（分布）を持ち、局所情報に基づいて独立に変形**する点が異なる。
- 共分散の更新則も CMA の自然勾配に基づいたものとは異なり、
  「疑似グラディエント方向での縮小＋直交方向での調整」という別の設計を採用している。

### 7.3 Nelder–Mead などの局所探索との比較

- Nelder–Mead は単一の単体を反射・収縮などで変形していく純局所探索。
- PGAS は複数の楕円体分布を並行に維持し、確率的サンプリングにより探索を行うため、
  多峰性のある関数に対して複数モードを追いかけることが期待できる。

---

## 8. ハイパーパラメータ設定の目安

- \( K \)：
  - 小次元・シンプルな問題：3〜5 ほど
  - 多峰性が予想される場合：10 前後まで増やすことも検討
- \( m \)：
  - 各クラスタに 20〜100 点程度。
  - メモリと評価回数制限を考慮しつつ調整。
- \( \alpha \)：
  - 0.05〜0.3 程度から試す。大き過ぎると不安定、小さ過ぎると収束が遅い。
- \( \beta \)：
  - 0.01〜0.2 程度。疑似グラディエントに対する形状変形の強さ。
- \( \gamma \)：
  - 0.1〜0.3 程度で「たまに探索モードに入る」イメージ。
- \( \sigma_\text{min}, \sigma_\text{max} \)：
  - 問題のスケールに依存。ボックス幅の 1〜5% を \( \sigma_\text{min} \)、
    20〜50% を \( \sigma_\text{max} \) のオーダーとするなど。

---

## 9. 拡張・バリエーション

### 9.1 制約条件付き最適化

- 制約違反度 \( c(x) \ge 0 \) を導入し、
  - まず「実行可能点のみ」でランキングして疑似グラディエントを構成
  - 実行可能点が少ない場合は、違反度の低い点を優先するようなランキングを設計
- ペナルティ関数法や可行性優先の比較ルールと組み合わせることも可能。

### 9.2 離散・混合変数

- 連続変数部分にのみ楕円体を定義し、
  離散変数はクラスタ内で頻度の高いカテゴリを優先しつつランダムに変化させる戦略を組み合わせる。
- あるいは、離散変数ごとにサブクラスタを持つような拡張も考えられる。

### 9.3 多目的最適化

- 各クラスタ内でパレートランクを計算し、
  - パレートフロントに属する点を「良い点」集合 \( G_k \) として疑似グラディエントを構成
  - あるいは crowding distance 等と組み合わせたランキングを使う
- 最終的には、履歴全体のパレートフロントを出力。

### 9.4 バッチ評価・並列化

- 1 イテレーションで複数クラスタから同時にサンプルし、
  並列に評価を行うバッチ版 PGAS も容易に定義可能。

---

## 10. 実装上のメモ

- Python 実装例：
  - `numpy` による線形代数
  - `scikit-learn` の `KMeans` によるクラスタリング
  - Cholesky 分解で楕円体サンプリング
- 数値安定性：
  - `S_k` は常に正定値となるように、更新のたびに
    - 正則化：`S_k += eps * I`
    - 固有値クリップ
- 評価ログ：
  - すべてのサンプルと評価値をログとして保存し、
    反復過程を可視化することで挙動の理解とデバッグが容易になる。

---

## 11. まとめ

本ドキュメントでは、

- 局所クラスタ
- ランキングに基づく疑似グラディエント
- サンプリング楕円体の適応的変形

という 3 要素を組み合わせた新しいブラックボックス最適化アルゴリズム
**Pseudo-Gradient Adaptive Sampling (PGAS)** を定義しました。

実用上は、

- 評価回数が限られる連続最適化問題
- 多峰性がある程度予想される状況
- サロゲートモデルを構築するほどのコストや前提を置きたくないケース

などでの応用が考えられます。

このままでも実装可能なレベルの仕様になっていますが、
実際の問題に合わせて

- クラスタリング戦略
- 疑似グラディエントの定義・平滑化
- 楕円体更新則のバリエーション

などを調整・発展させる余地があります。
