# BISTRO: Bandit-allocated Implicit Surrogate Trust-Region Optimizer

## 1. 問題設定

ブラックボックス関数
\[
f : \Omega \subset \mathbb{R}^d \to \mathbb{R}
\]
の最小化を考える。

- 勾配は利用できない
- 評価コストが高い（クエリ効率が重要）
- 評価値にはノイズが乗っていてもよい
- 連続変数を主対象とし、離散・混合への拡張も想定する

---

## 2. アルゴリズム概要

**BISTRO（Bandit-allocated Implicit Surrogate Trust-Region Optimizer）**は、

1. 探索空間内に **複数のトラストリージョン（TR）** を配置する
2. 各 TR 周辺の履歴評価点から、**暗黙的な局所サロゲート（ローカル統計モデル）** を構成する
3. その局所平均・分散から **LCB のような指標** を計算し、
4. **多腕バンディット的に評価資源を各 TR に割り当てる**
5. 各 TR 内ではローカルサンプリングとトラストリージョン更新則で局所探索を行う

という枠組みを取る。

ポイントは以下の 3 つの組み合わせにある。

- 複数の局所トラストリージョン（multi-start の一般化）
- 暗黙的・軽量な局所サロゲート（kNN / カーネルスムージング）
- 多腕バンディットによる **「どの局所を深掘りするか」** の資源配分

GP ベースの Bayesian Optimization ほど重くなく、CMA-ES のような単一分布でもない、中間的な実用的アルゴリズムを狙っている。

---

## 3. 内部状態

### 3.1 評価履歴

ステップ \(t\) までの評価履歴を
\[
\mathcal{D}_t = \{(x_i, y_i)\}_{i=1}^{N_t}, \quad y_i = f(x_i)
\]
として保持する。

### 3.2 K 個のトラストリージョン

領域番号 \(k = 1, \dots, K\) ごとに以下を持つ。

- 中心: \(c_k \in \Omega\)
- 半径: \(r_k > 0\)
- 局所スケール（カーネル幅）: \(h_k > 0\)
- 状態変数: 成功/失敗カウンタ、停滞回数、局所ベスト値など

トラストリージョンは
\[
\text{TR}_k = \{ x \in \Omega : \|x - c_k\|_2 \le r_k \}
\]
と定義する。必要に応じて、楕円体
\[
\{x : (x - c_k)^\top M_k (x - c_k) \le 1\}
\]
のような一般化も可能である。

---

## 4. 暗黙的局所サロゲート

各 TR \(k\) の中心 \(c_k\) 周りで、履歴データから **重み付き局所平均・分散** を計算する。

### 4.1 近傍点集合

\[
\mathcal{N}_k = \{(x_i, y_i) \in \mathcal{D}_t : \|x_i - c_k\|_2 \le \alpha h_k\}
\]
ここで \(\alpha\) は 2〜3 程度の定数。

実装上は「半径制約」ではなく「k 近傍」（例: max(10, 2d) 点）で定義してもよい。

### 4.2 カーネル重み

たとえばガウシアンカーネルを用いる：
\[
w_i = \exp\left(-\frac{\|x_i - c_k\|_2^2}{2 h_k^2}\right).
\]

正規化重み
\[
\tilde{w}_i = \frac{w_i}{\sum_{j\in\mathcal{N}_k} w_j + \epsilon}
\]
を用いて局所平均・分散を定義する。

### 4.3 局所平均・分散推定

- 局所平均
  \[
  \hat{\mu}_k = \sum_{i\in\mathcal{N}_k} \tilde{w}_i y_i
  \]

- 局所分散
  \[
  \hat{\sigma}_k^2 = \sum_{i\in\mathcal{N}_k} \tilde{w}_i (y_i - \hat{\mu}_k)^2 + \sigma_0^2
  \]
  ここで \(\sigma_0^2\) は観測ノイズを表す小さな定数。

このように、明示的な GP や NN の学習を行わず、**履歴データの統計量のみで局所サロゲートを構成**する。

---

## 5. バンディット型資源配分

各トラストリージョンの「評価すべき度」を表すスコアを定義し、多腕バンディットのように評価資源を割り当てる。

### 5.1 LCB 指標

最小化問題を考えるので、Lower Confidence Bound 型の指標
\[
\text{LCB}_k = \hat{\mu}_k - \lambda \hat{\sigma}_k
\]
を定義する（\(\lambda > 0\) は探索・活用のバランスを決めるハイパーパラメータ）。

- \(\hat{\mu}_k\) が小さいほど「期待される目的値が低く、良さそう」
- \(\hat{\sigma}_k\) が大きいほど「不確実性が高く、探索に値する」

### 5.2 領域の選択（ε-greedy / ソフトマックス）

1 回の反復で B 個の評価（バッチ）を行えるとすると、b = 1, ..., B について以下を行う。

- ε-greedy:
  - 確率 \(1 - \epsilon\): \(\text{LCB}_k\) が最小の k を選択
  - 確率 \(\epsilon\): 一様ランダムに領域を選択

- あるいはソフトマックス（Boltzmann）サンプリング:
  \[
  p_k \propto \exp(-\eta \cdot \text{LCB}_k)
  \]
  で k をランダムにサンプリングする（\(\eta > 0\) は温度）。

---

## 6. トラストリージョン内のサンプリング

選択された TR \(k\) から新しい評価点 \(x_{\text{new}}\) を生成する。

### 6.1 一様球サンプリング

- 単位球内の一様分布から \(u\) をサンプル
- \(x_{\text{new}} = c_k + r_k \, u\)

### 6.2 ガウシアンサンプリング

- \(z \sim \mathcal{N}(0, I)\)
- \(x_{\text{new}} = c_k + s \, z\)（s は適当なスケール）
- \(x_{\text{new}}\) が TR の外に出た場合はリジェクトして再サンプルするか、境界上に射影する

### 6.3 制約処理

- \(\Omega\) がボックス制約の場合、サンプル後にクリッピング
- 一般の制約付き問題では、ペナルティ法や可行性優先ルールと組み合わせる

---

## 7. トラストリージョンの更新

各領域に対して、最近の評価結果をもとに成功/失敗判定を行い、中心や半径を更新する。

### 7.1 改善量の評価

領域 k について

- \(y_{\text{best}}^{\text{old},k}\): 更新前にその TR 内で得られていたベスト値
- \(y_{\text{best}}^{\text{new},k}\): 新しい評価を含めたベスト値

とし、改善量を
\[
\Delta_k = y_{\text{best}}^{\text{old},k} - y_{\text{best}}^{\text{new},k}
\]
と定義する。

### 7.2 成功/失敗ルールの例

- **成功（十分な改善）**
  \[
  \Delta_k > \kappa_{\text{succ}} \cdot r_k
  \]
  のとき
  - 中心を新ベスト点へ移動: \(c_k \leftarrow x_{\text{best}}^k\)
  - 半径を縮小: \(r_k \leftarrow \gamma_{\text{dec}} r_k\)（例：\(\gamma_{\text{dec}} = 0.5\)）
  - カーネル幅も縮小: \(h_k \leftarrow \gamma_{\text{dec}} h_k\)
  - 成功カウンタをリセット or 増加

- **失敗（改善が小さい/無い）**
  \[
  \Delta_k \le \kappa_{\text{succ}} \cdot r_k
  \]
  のとき
  - 半径を拡大: \(r_k \leftarrow \gamma_{\text{inc}} r_k\)（例：\(\gamma_{\text{inc}} = 1.5\)）
  - 停滞カウンタを増加

### 7.3 リスタート条件

以下のいずれかを満たした場合、TR を「リスタート」する。

- \(r_k > r_{\max}\)
- 停滞カウンタがある閾値を超えた
- 一定回数以上連続して失敗

リスタート時には

1. グローバルに新しい中心候補 \(c_k\) をサンプル
   - 既知の良い点群からランダムサンプル
   - あるいは単純に一様サンプル
2. \(r_k \leftarrow r_0\), \(h_k \leftarrow r_0\)
3. ローカル履歴・カウンタをリセット

こうして、局所探索とグローバルリスタートを自然に統合できる。

---

## 8. 擬似コード

以下は連続最小化の場合の高レベルな擬似コードである。

```text
Input:
  領域 Ω, 初期サンプル数 N0, トラストリージョン数 K,
  バッチサイズ B,
  ハイパーパラメータ (λ, ε, η, r0, r_max, γ_dec, γ_inc, κ_succ, ...)

1: 初期化
   - Latin hypercube 等で N0 点 {x_i} をサンプルし f(x_i) を評価
   - D ← {(x_i, y_i)}
   - D のうち目的値が良い点の上位 K 点、またはクラスタリング中心などを
     用いて c_k (k=1..K) を決める
   - 各 k について r_k ← r0, h_k ← r0

2: for t = 1, 2, ... until 評価予算を使い切る:
3:     # 各 TR の局所サロゲート更新
4:     for k = 1..K:
5:         近傍集合 N_k を構成 (半径 or kNN)
6:         μ̂_k, σ̂_k を計算
7:         LCB_k = μ̂_k - λ σ̂_k

8:     # バッチ生成
9:     for b = 1..B:
10:        with prob ε:
11:            k ← 一様ランダム({1,...,K})
12:        else:
13:            k ← argmin_k LCB_k
              # あるいは softmax(L C B) によるサンプリング
14:        x_new ← TR_k (中心 c_k, 半径 r_k) からサンプル
15:        y_new ← f(x_new)
16:        D ← D ∪ {(x_new, y_new)}
17:        TR_k の最近の評価リストを更新

18:     # トラストリージョン更新
19:     for k = 1..K:
20:         y_best_old^k, y_best_new^k から Δ_k を計算
21:         if Δ_k > κ_succ r_k:  # 成功
22:             c_k ← x_best^k
23:             r_k ← γ_dec r_k
24:             h_k ← γ_dec h_k
25:             停滞カウンタのリセット
26:         else:                 # 失敗
27:             r_k ← γ_inc r_k
28:             停滞カウンタ++

29:         if r_k > r_max or 停滞カウンタ > 閾値:
30:             c_k をグローバルに再サンプル
31:             r_k ← r0, h_k ← r0
32:             カウンタをリセット

Output:
   D の中で y_i が最小となる x_i を解として返す
```

---

## 9. 実用上のパラメータ設定の目安

- **TR 数 K**
  - 5〜20 程度。次元数や評価予算に応じて調整する。
  - 次元が高く評価予算が小さい場合は K を小さめに。
- **λ（LCB の探索係数）**
  - 0.5〜2.0 の範囲でグリッドサーチまたは経験的調整。
- **ε（ε-greedy のランダム比率）**
  - 0.05〜0.2 程度。
- **初期半径 r0**
  - 各変数の許容範囲の 10〜20% 程度。
- **最大半径 r_max**
  - 許容範囲全体の 50〜100% 程度。
- **近傍点数**
  - 各 TR で kNN を利用するなら max(10, 2d) 程度を目安にする。

---

## 10. 拡張・バリエーション

- **楕円体 TR**
  - 各 TR にメトリック行列 \(M_k\) を持たせ、
    \((x - c_k)^\top M_k (x - c_k) \le 1\)
    となるような楕円体にすることで、異方的な探索も可能。
- **Thompson Sampling 型の領域選択**
  - LCB ではなく \(\hat{\mu}_k\) と \(\hat{\sigma}_k\) から簡易的にサンプルを生成し、
    その「サンプルされた目的値」が最小の領域を選択するなど。
- **離散・混合変数への対応**
  - 離散変数に対してハミング距離ベースのカーネルを導入し、
    連続・離散を組み合わせた距離関数を定義する。
  - サンプリング時には連続部分は現在の枠組みどおり、離散部分はカテゴリカルサンプルとする。
- **制約付き最適化**
  - 制約違反度をペナルティとして目的関数に加える、
  - あるいは可行解を優先して保持するルールを追加することで拡張できる。

---

## 11. まとめ

BISTRO は、

- **複数のトラストリージョン**
- **暗黙的な局所サロゲート（kNN / カーネルスムージング）**
- **多腕バンディットによる資源配分**

を組み合わせた、新しい実用的ブラックボックス最適化アルゴリズムの枠組みである。

- GP ベース BO より軽量で実装も容易
- CMA-ES などの単一分布ベースとは異なり、複数局所を同時に管理
- ノイズや並列評価にも自然に対応

実装レベルでは、上記の擬似コードをそのまま Python + NumPy / JAX / PyTorch などで写経すればよく、  
各種ハイパーパラメータはタスクに応じて調整すればよい。
